---
layout: post
title:  "Understanding appetizers can improve your hiring funnel"
image: /assets/images/swiss-cheese_header.jpg
---

# The Swiss Cheese dynamics

This is an old "joke", originally called "swiss cheese paradox". It's on the
Internet at least since 1998, but I remember hearing it as a child so it's
probably older. It goes as follows:

> Consider the Swiss Cheese:
>
>The more holes you have, the less cheese you have.
>
>The more cheese you have, the more holes you have.
>
>The more cheese you have, the less cheese you have.

Explaining a joke is no fun, but it's important this time. It's true that the
more *holes per volume* of cheese, the less *mass* of cheese you have. It's also
true that the more *volume* of cheese, the more *number of holes*. Now the more
*volume* of cheese, *holes per volume* remains constant; so the conclusion
doesn't follow.

# The size of a cheese platter

In recruiting, we have a similar but more subtle Swiss Cheese dynamic:

> Consider the talent pool for a job:
>
>The more mismatches you have, the less screening you make.
>
>The more applicants you have, the more mismatches you have.
>
>The more applicants you have, the less screening you make.

At first, more applicants means that you need to be more selective. If you have
1 open position, you might want to screen 10 candidates. If you have 20
applicants, you'll observe 50% of them. Even randomly selecting 10 applicants
will give you a 50% chance of screening the the one that yields the best match
with the job. If you have 1000 applicants, however, you'll observe 1% of them,
so randomly selecting 10 applicants will give a 1% chance of accomplishing the
the best match. This means that you have to proportionally increase screening.

Or does it not?

Should you really measure success with focus on the best candidate? Can you even
objectively measure the "best", having that much diversity in a talent pool? In
20 applicants, one expects the difference between "a best" and "a second best"
to be noticeable, but in 10000 applicants there ought to be a handful of
candidates for all levels of matching. This means that screening doesn't have to
be different: the match/mismatch ratio is still the same, just like the
cheese/hole density as you get more Swiss Cheese in the platter.

So should we observe more or less candidates when we have a bigger pool?
Neither. Considering a random selection, the expected matching of the observed
candidates is the average matching of whole pool. Given constant average
matching, more positions means more screening but more applicants means nothing.

# The label says this is Emmental cheese

The argument was made regarding a totally random selection for screening. This
is approximately the case for most workflows, where recruiters observe
candidates in an order without much meaning - e.g. application timestamps or
candidate names and other usual sorting criteria on "analogic" processes.

What if selection was not random? If candidates can be automatically sorted
before screening, say with tests and profile assessments, does the conclusion
still hold? It does not, a very simple example can show why.

Let's say every applicant is automatically given a profile assessment test that
yields a fitness score for the job that you can use as sorting criteria.

TODO... test if good ordering yields less screening

# The lab says this is Jarslberg cheese

TODO... tests are automated screening. But we can do better.
